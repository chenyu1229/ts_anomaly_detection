{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6de956f-50ac-4568-9b78-da3324b831dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import layers\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "def read_dataset(file):\n",
    "    df = pd.read_csv(file, parse_dates=True, index_col=\"timestamp\",header=0)\n",
    "    return df\n",
    "\n",
    "# Generated training sequences for use in the model.\n",
    "def create_sequences(values, time_steps):\n",
    "    output = []\n",
    "    for i in range(len(values) - time_steps + 1):\n",
    "        output.append(values[i : (i + time_steps)])\n",
    "    return np.stack(output)\n",
    "\n",
    "def pre_processing(df,time_steps):\n",
    "    df = df.dropna()\n",
    "    training_mean = df.mean()\n",
    "    training_std = df.std()\n",
    "    df_training_value = (df - training_mean) / training_std\n",
    "    print(\"Number of training samples:\", len(df_training_value))\n",
    "    x_train = create_sequences(df_training_value,time_steps)\n",
    "    print(\"Training input shape: \", x_train.shape)\n",
    "    return x_train, training_mean, training_std\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# x_train = create_sequences(df_training_value)\n",
    "# print(\"Training input shape: \", x_train.shape)\n",
    "def build_model(x_train):\n",
    "    n_steps = x_train.shape[1]\n",
    "    n_features = x_train.shape[2]\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            layers.Input(shape=(n_steps, n_features)),\n",
    "            layers.Conv1D(filters=32, kernel_size=15, padding='same', data_format='channels_last',\n",
    "                dilation_rate=1, activation=\"linear\"),\n",
    "            layers.LSTM(\n",
    "                units=25, activation=\"tanh\", name=\"lstm_1\", return_sequences=False\n",
    "            ),\n",
    "            layers.RepeatVector(n_steps),\n",
    "            layers.LSTM(\n",
    "                units=25, activation=\"tanh\", name=\"lstm_2\", return_sequences=True\n",
    "            ),\n",
    "            layers.Conv1D(filters=32, kernel_size=15, padding='same', data_format='channels_last',\n",
    "                dilation_rate=1, activation=\"linear\"),\n",
    "            layers.TimeDistributed(layers.Dense(x_train.shape[2], activation='linear'))\n",
    "        ]\n",
    "    )\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=\"mse\")\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def save_res(train_mae_loss,training_mean,training_std):\n",
    "    plt.hist(train_mae_loss, bins=50)\n",
    "    plt.xlabel(\"Training MAE\")\n",
    "    plt.ylabel(\"No of samples\")\n",
    "    plt.savefig(\"../res/training_MAE_loss.jpg\")\n",
    "\n",
    "    # Get reconstruction loss threshold.\n",
    "    threshold = np.max(train_mae_loss)\n",
    "    mesg = \"threshold=\"+str(threshold)+\"\\n\"+\"training_mean=\"+str(list(training_mean))+\"\\n\"+\"training_std=\"+str(list(training_std))+\"\\n\"+\"time_steps=\"+str(int(TIME_STEPS))\n",
    "    print(\"Reconstruction error threshold: \", threshold)\n",
    "\n",
    "    model.save('../res/model.keras') \n",
    "    with open(\"training_var.py\",\"w\") as f:\n",
    "        f.write(mesg)\n",
    "    print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1de1cc37-14fd-499d-bf16-3ae467a7958b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168\n",
      "Number of training samples: 13896\n",
      "Training input shape:  (13729, 168, 2)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 168, 32)           992       \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 25)                5800      \n",
      "                                                                 \n",
      " repeat_vector (RepeatVector  (None, 168, 25)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 168, 25)           5100      \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 168, 32)           12032     \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 168, 2)           66        \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,990\n",
      "Trainable params: 23,990\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "97/97 [==============================] - 17s 139ms/step - loss: 0.9005 - val_loss: 0.6976\n",
      "Epoch 2/2\n",
      "97/97 [==============================] - 13s 130ms/step - loss: 0.5295 - val_loss: 0.2415\n"
     ]
    }
   ],
   "source": [
    "TIME_STEPS = int(24*7)\n",
    "# if len(sys.argv) > 1:\n",
    "#     TIME_STEPS = int(sys.argv[1])\n",
    "print (TIME_STEPS)\n",
    "df = read_dataset('../data/train.csv')\n",
    "x_train, training_mean, training_std = pre_processing(df,TIME_STEPS)\n",
    "model  = build_model(x_train)\n",
    "\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    x_train,\n",
    "    epochs=2,\n",
    "    batch_size=128,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=25, mode=\"min\")\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9e4d4bf-473e-4a3c-9de5-81546cea27d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Training MAE Loss\n",
      "430/430 [==============================] - 9s 19ms/step\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculating Training MAE Loss\")\n",
    "x_train_pred = model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0c737c4-69ad-4380-a0c2-13b3628582f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13729, 168, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77be553d-f550-4d2e-be59-bdfdc7a08476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.04893118,  0.04893118],\n",
       "        [-0.16852572, -0.16852572],\n",
       "        [ 0.27973701,  0.27973701],\n",
       "        ...,\n",
       "        [ 0.12367963,  0.12367963],\n",
       "        [ 0.13097225,  0.13097225],\n",
       "        [-0.50846141, -0.50846141]],\n",
       "\n",
       "       [[-0.16852572, -0.16852572],\n",
       "        [ 0.27973701,  0.27973701],\n",
       "        [ 0.24056689,  0.24056689],\n",
       "        ...,\n",
       "        [ 0.13097225,  0.13097225],\n",
       "        [-0.50846141, -0.50846141],\n",
       "        [-0.18622233, -0.18622233]],\n",
       "\n",
       "       [[ 0.27973701,  0.27973701],\n",
       "        [ 0.24056689,  0.24056689],\n",
       "        [ 0.21685399,  0.21685399],\n",
       "        ...,\n",
       "        [-0.50846141, -0.50846141],\n",
       "        [-0.18622233, -0.18622233],\n",
       "        [-0.04874744, -0.04874744]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-0.97640539, -0.97640539],\n",
       "        [-1.37220477, -1.37220477],\n",
       "        [-1.39513144, -1.39513144],\n",
       "        ...,\n",
       "        [-1.37610001, -1.37610001],\n",
       "        [-1.13946177, -1.13946177],\n",
       "        [-0.96459216, -0.96459216]],\n",
       "\n",
       "       [[-1.37220477, -1.37220477],\n",
       "        [-1.39513144, -1.39513144],\n",
       "        [-1.43242957, -1.43242957],\n",
       "        ...,\n",
       "        [-1.13946177, -1.13946177],\n",
       "        [-0.96459216, -0.96459216],\n",
       "        [-1.01093625, -1.01093625]],\n",
       "\n",
       "       [[-1.39513144, -1.39513144],\n",
       "        [-1.43242957, -1.43242957],\n",
       "        [-0.80422969, -0.80422969],\n",
       "        ...,\n",
       "        [-0.96459216, -0.96459216],\n",
       "        [-1.01093625, -1.01093625],\n",
       "        [-1.28840381, -1.28840381]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d141517-86cc-429a-a946-f483828078d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.09696697, -0.11046512],\n",
       "        [-0.09821392, -0.09343665],\n",
       "        [-0.08766912, -0.08505655],\n",
       "        ...,\n",
       "        [-0.3676481 , -0.31670105],\n",
       "        [-0.3318029 , -0.26556945],\n",
       "        [-0.34177592, -0.31342804]],\n",
       "\n",
       "       [[-0.10053995, -0.11290564],\n",
       "        [-0.09985053, -0.09447443],\n",
       "        [-0.08751035, -0.08430521],\n",
       "        ...,\n",
       "        [-0.4012837 , -0.34554672],\n",
       "        [-0.36277148, -0.29588363],\n",
       "        [-0.3724099 , -0.33678803]],\n",
       "\n",
       "       [[-0.10165723, -0.11661149],\n",
       "        [-0.0987596 , -0.09680897],\n",
       "        [-0.08376426, -0.08532243],\n",
       "        ...,\n",
       "        [-0.35032192, -0.29477543],\n",
       "        [-0.31858495, -0.25272712],\n",
       "        [-0.33867875, -0.2990642 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-0.1752049 , -0.13734207],\n",
       "        [-0.32393107, -0.2873908 ],\n",
       "        [-0.4699705 , -0.42373547],\n",
       "        ...,\n",
       "        [-0.08516219, -0.11436974],\n",
       "        [ 0.0112204 ,  0.0310663 ],\n",
       "        [ 0.24094394,  0.26747409]],\n",
       "\n",
       "       [[-0.19703773, -0.16066372],\n",
       "        [-0.35400644, -0.31685108],\n",
       "        [-0.50661176, -0.45973745],\n",
       "        ...,\n",
       "        [-0.19453402, -0.21377358],\n",
       "        [-0.08062364, -0.05798711],\n",
       "        [ 0.1690483 ,  0.19332199]],\n",
       "\n",
       "       [[-0.2087177 , -0.17445654],\n",
       "        [-0.3684651 , -0.33124208],\n",
       "        [-0.52265286, -0.47507545],\n",
       "        ...,\n",
       "        [-0.22562608, -0.236605  ],\n",
       "        [-0.10612245, -0.07946604],\n",
       "        [ 0.15073204,  0.17375468]]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0894af4a-2eea-42dc-b55f-a32a4ca148ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mae_loss = np.mean((np.mean(np.abs(x_train_pred - x_train), axis=1)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b2e922f-a89e-4417-b414-49f7f393a996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13729,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mae_loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1f83624-558d-43b8-8d52-162c64b7f616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction error threshold:  0.8961955094102918\n",
      "Model Saved\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "save_res(train_mae_loss,training_mean,training_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c431e3e-2112-4822-a8f2-18e7805a000c",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = np.max(train_mae_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6ad5afb-1632-403e-b969-0131b1d1bda1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8961955094102918"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806cb193-5fe3-4287-98f1-94178d850a9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7194e8-8f43-4cba-81d9-84c8d3d96ff1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf76a620-dba1-4651-970d-a9752810488e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee8a9b2e-415f-4a92-9391-a706895bdf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras import layers\n",
    "from matplotlib import pyplot as plt\n",
    "import training_var\n",
    "import sys\n",
    "\n",
    "\n",
    "def read_dataset(file):\n",
    "    df = pd.read_csv(file, parse_dates=True, index_col=\"timestamp\",header=0)\n",
    "    return df\n",
    "\n",
    "# Generated training sequences for use in the model.\n",
    "def create_sequences(values, time_steps):\n",
    "    output = []\n",
    "    for i in range(len(values) - time_steps + 1):\n",
    "        output.append(values[i : (i + time_steps)])\n",
    "    return np.stack(output)\n",
    "\n",
    "def pre_processing(df,TIME_STEPS):\n",
    "    df = df.dropna()\n",
    "    df_test_value = (df - training_var.training_mean)/ training_var.training_std\n",
    "    # Create sequences from test values.\n",
    "    x_test = create_sequences(df_test_value.values,TIME_STEPS)\n",
    "    print(\"Test input shape: \", x_test.shape)\n",
    "    return x_test\n",
    "\n",
    "def univariate_anomalous_data(anomalies):\n",
    "    anomalous_data_indices_raw = np.where(anomalies)[0]\n",
    "    print(\"raw anomaly samples: \", anomalous_data_indices_raw)\n",
    "    anomalous_data_indices_raw = [x+TIME_STEPS/2 for x in anomalous_data_indices_raw]\n",
    "    anomalous_data_indices = []\n",
    "    for data_idx in anomalous_data_indices_raw:\n",
    "        if set(range(int(data_idx)-int(TIME_STEPS/4),int(data_idx)+int(TIME_STEPS/4))).issubset(set(anomalous_data_indices_raw)):\n",
    "            anomalous_data_indices.append(int(data_idx))\n",
    "    print(\"Number of anomaly samples: \", len(anomalous_data_indices))\n",
    "    print(\"Indices of anomaly samples: \", anomalous_data_indices)\n",
    "    return anomalous_data_indices\n",
    "\n",
    "def generate_res(df_test):\n",
    "    df_res = df_test.reset_index()\n",
    "    df_res['res'] = False\n",
    "    df_res.loc[anomalous_data_indices,'res'] = True\n",
    "    df_res = df_res['res'] \n",
    "    df_res.to_csv(\"../res/res.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33034cb2-3100-4296-add2-039cb755a42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test input shape:  (853, 168, 2)\n"
     ]
    }
   ],
   "source": [
    "TIME_STEPS = training_var.time_steps\n",
    "df_test = read_dataset('../data/test.csv')\n",
    "# df_test = pd.read_csv('../data/test.csv',parse_dates=True, index_col=\"timestamp\",header=0)\n",
    "x_test = pre_processing(df_test,TIME_STEPS)\n",
    "model = keras.models.load_model('../res/model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f093a2a-23b4-42ba-8055-7558cda53f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 1s 20ms/step\n"
     ]
    }
   ],
   "source": [
    "x_test_pred = model.predict(x_test)\n",
    "test_mae_loss = np.mean(np.abs(x_test_pred - x_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dffae344-35b2-4e3d-95ca-ee95584c70a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(853, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mae_loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7728e472-deb0-447f-98dc-797b01edf4a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.24583492],\n",
       "       [0.24530709],\n",
       "       [0.23988283],\n",
       "       [0.23530061],\n",
       "       [0.23466073],\n",
       "       [0.23211747],\n",
       "       [0.22691829],\n",
       "       [0.22562859],\n",
       "       [0.2221147 ],\n",
       "       [0.22206621],\n",
       "       [0.21734948],\n",
       "       [0.21369621],\n",
       "       [0.21219807],\n",
       "       [0.20530635],\n",
       "       [0.20925745],\n",
       "       [0.20922043],\n",
       "       [0.21850484],\n",
       "       [0.22283909],\n",
       "       [0.22506483],\n",
       "       [0.2240477 ],\n",
       "       [0.21628623],\n",
       "       [0.21456709],\n",
       "       [0.21228801],\n",
       "       [0.21469267],\n",
       "       [0.21637185],\n",
       "       [0.2160269 ],\n",
       "       [0.21514701],\n",
       "       [0.21252809],\n",
       "       [0.21157097],\n",
       "       [0.20998351],\n",
       "       [0.20972647],\n",
       "       [0.21158937],\n",
       "       [0.21450477],\n",
       "       [0.21988985],\n",
       "       [0.21789274],\n",
       "       [0.21321649],\n",
       "       [0.21044902],\n",
       "       [0.21064248],\n",
       "       [0.21048559],\n",
       "       [0.20325651],\n",
       "       [0.19998314],\n",
       "       [0.20609971],\n",
       "       [0.20299504],\n",
       "       [0.19953579],\n",
       "       [0.20270707],\n",
       "       [0.21126766],\n",
       "       [0.21492309],\n",
       "       [0.21378971],\n",
       "       [0.2146587 ],\n",
       "       [0.21433214],\n",
       "       [0.21472407],\n",
       "       [0.21683909],\n",
       "       [0.22236155],\n",
       "       [0.22167039],\n",
       "       [0.22548078],\n",
       "       [0.23255708],\n",
       "       [0.23167951],\n",
       "       [0.22859828],\n",
       "       [0.23185366],\n",
       "       [0.2356097 ],\n",
       "       [0.23472874],\n",
       "       [0.23392556],\n",
       "       [0.23883786],\n",
       "       [0.23868654],\n",
       "       [0.23915626],\n",
       "       [0.24214324],\n",
       "       [0.24725294],\n",
       "       [0.25227506],\n",
       "       [0.26161532],\n",
       "       [0.26614272],\n",
       "       [0.27206954],\n",
       "       [0.2761991 ],\n",
       "       [0.27556014],\n",
       "       [0.27606891],\n",
       "       [0.27951069],\n",
       "       [0.28270209],\n",
       "       [0.27934656],\n",
       "       [0.27622293],\n",
       "       [0.26516228],\n",
       "       [0.2884754 ],\n",
       "       [0.29263332],\n",
       "       [0.28255872],\n",
       "       [0.26825445],\n",
       "       [0.25576166],\n",
       "       [0.2431399 ],\n",
       "       [0.23584568],\n",
       "       [0.23243665],\n",
       "       [0.22819484],\n",
       "       [0.2260979 ],\n",
       "       [0.22332212],\n",
       "       [0.2206949 ],\n",
       "       [0.21980879],\n",
       "       [0.22015464],\n",
       "       [0.21741501],\n",
       "       [0.22251861],\n",
       "       [0.23209467],\n",
       "       [0.23299322],\n",
       "       [0.2420725 ],\n",
       "       [0.24635127],\n",
       "       [0.24331325],\n",
       "       [0.24367581],\n",
       "       [0.24515444],\n",
       "       [0.24807041],\n",
       "       [0.25170487],\n",
       "       [0.25144939],\n",
       "       [0.24849369],\n",
       "       [0.24685372],\n",
       "       [0.24621791],\n",
       "       [0.24916162],\n",
       "       [0.25124306],\n",
       "       [0.24729674],\n",
       "       [0.24821025],\n",
       "       [0.24813955],\n",
       "       [0.24350111],\n",
       "       [0.24218581],\n",
       "       [0.23930499],\n",
       "       [0.23812955],\n",
       "       [0.23048722],\n",
       "       [0.22639548],\n",
       "       [0.2232339 ],\n",
       "       [0.21866891],\n",
       "       [0.21904719],\n",
       "       [0.21693146],\n",
       "       [0.22187905],\n",
       "       [0.22215839],\n",
       "       [0.22164213],\n",
       "       [0.22101005],\n",
       "       [0.21917374],\n",
       "       [0.22054873],\n",
       "       [0.22262033],\n",
       "       [0.21890063],\n",
       "       [0.21896111],\n",
       "       [0.21702823],\n",
       "       [0.21639125],\n",
       "       [0.22093521],\n",
       "       [0.22286772],\n",
       "       [0.22725453],\n",
       "       [0.23148774],\n",
       "       [0.23482512],\n",
       "       [0.23714346],\n",
       "       [0.23997237],\n",
       "       [0.24533949],\n",
       "       [0.25130187],\n",
       "       [0.25251844],\n",
       "       [0.24986269],\n",
       "       [0.24951943],\n",
       "       [0.24930309],\n",
       "       [0.25046706],\n",
       "       [0.2474835 ],\n",
       "       [0.24661469],\n",
       "       [0.24822193],\n",
       "       [0.24541725],\n",
       "       [0.24565194],\n",
       "       [0.24440719],\n",
       "       [0.24082239],\n",
       "       [0.24050933],\n",
       "       [0.24015024],\n",
       "       [0.2371234 ],\n",
       "       [0.23663923],\n",
       "       [0.23766316],\n",
       "       [0.23846875],\n",
       "       [0.23818705],\n",
       "       [0.24422367],\n",
       "       [0.24695889],\n",
       "       [0.2489544 ],\n",
       "       [0.25042682],\n",
       "       [0.24977987],\n",
       "       [0.24800463],\n",
       "       [0.24701591],\n",
       "       [0.2459488 ],\n",
       "       [0.24505243],\n",
       "       [0.24572723],\n",
       "       [0.2438051 ],\n",
       "       [0.24516964],\n",
       "       [0.24560184],\n",
       "       [0.24497079],\n",
       "       [0.24767623],\n",
       "       [0.24627876],\n",
       "       [0.24587139],\n",
       "       [0.24404297],\n",
       "       [0.23106815],\n",
       "       [0.22393966],\n",
       "       [0.21671518],\n",
       "       [0.22028999],\n",
       "       [0.22491005],\n",
       "       [0.23296133],\n",
       "       [0.2344555 ],\n",
       "       [0.23235987],\n",
       "       [0.22946367],\n",
       "       [0.22935227],\n",
       "       [0.22415006],\n",
       "       [0.22130156],\n",
       "       [0.22274906],\n",
       "       [0.22558099],\n",
       "       [0.23012984],\n",
       "       [0.2335522 ],\n",
       "       [0.23595448],\n",
       "       [0.23508883],\n",
       "       [0.23627968],\n",
       "       [0.23661275],\n",
       "       [0.24206982],\n",
       "       [0.23615388],\n",
       "       [0.23479308],\n",
       "       [0.22914669],\n",
       "       [0.2255309 ],\n",
       "       [0.22399697],\n",
       "       [0.22077889],\n",
       "       [0.2180129 ],\n",
       "       [0.2177501 ],\n",
       "       [0.21637722],\n",
       "       [0.21805972],\n",
       "       [0.21774036],\n",
       "       [0.21870359],\n",
       "       [0.21684859],\n",
       "       [0.21690089],\n",
       "       [0.21797954],\n",
       "       [0.222215  ],\n",
       "       [0.22633535],\n",
       "       [0.22986393],\n",
       "       [0.23049056],\n",
       "       [0.23407483],\n",
       "       [0.23820138],\n",
       "       [0.24248909],\n",
       "       [0.24518668],\n",
       "       [0.24871482],\n",
       "       [0.25305854],\n",
       "       [0.25337559],\n",
       "       [0.25195261],\n",
       "       [0.2498157 ],\n",
       "       [0.24672973],\n",
       "       [0.23816626],\n",
       "       [0.23362109],\n",
       "       [0.22678821],\n",
       "       [0.22086246],\n",
       "       [0.21904081],\n",
       "       [0.2196389 ],\n",
       "       [0.2229463 ],\n",
       "       [0.22434159],\n",
       "       [0.22793454],\n",
       "       [0.23565654],\n",
       "       [0.24092513],\n",
       "       [0.24402681],\n",
       "       [0.24258761],\n",
       "       [0.23963409],\n",
       "       [0.24000005],\n",
       "       [0.23653359],\n",
       "       [0.237586  ],\n",
       "       [0.24197012],\n",
       "       [0.24727083],\n",
       "       [0.25911312],\n",
       "       [0.2671595 ],\n",
       "       [0.27137573],\n",
       "       [0.27240104],\n",
       "       [0.27470817],\n",
       "       [0.27803865],\n",
       "       [0.24425241],\n",
       "       [0.2316367 ],\n",
       "       [0.23742143],\n",
       "       [0.23627464],\n",
       "       [0.23427205],\n",
       "       [0.23474633],\n",
       "       [0.23555794],\n",
       "       [0.23364316],\n",
       "       [0.24291839],\n",
       "       [0.24750041],\n",
       "       [0.24596011],\n",
       "       [0.24276616],\n",
       "       [0.23985765],\n",
       "       [0.24249205],\n",
       "       [0.26381679],\n",
       "       [0.26744155],\n",
       "       [0.26766654],\n",
       "       [0.26356075],\n",
       "       [0.25751391],\n",
       "       [0.24993937],\n",
       "       [0.24875363],\n",
       "       [0.24760077],\n",
       "       [0.24209812],\n",
       "       [0.23691033],\n",
       "       [0.23442248],\n",
       "       [0.23210404],\n",
       "       [0.23841769],\n",
       "       [0.24663052],\n",
       "       [0.24647485],\n",
       "       [0.24546984],\n",
       "       [0.24541421],\n",
       "       [0.24425014],\n",
       "       [0.24510568],\n",
       "       [0.2461861 ],\n",
       "       [0.24813819],\n",
       "       [0.24937961],\n",
       "       [0.24877499],\n",
       "       [0.24994065],\n",
       "       [0.24627551],\n",
       "       [0.24141346],\n",
       "       [0.241536  ],\n",
       "       [0.24519804],\n",
       "       [0.24957053],\n",
       "       [0.25560468],\n",
       "       [0.25244667],\n",
       "       [0.25689308],\n",
       "       [0.25572962],\n",
       "       [0.25770097],\n",
       "       [0.26493004],\n",
       "       [0.25663622],\n",
       "       [0.26001047],\n",
       "       [0.25134639],\n",
       "       [0.24210942],\n",
       "       [0.24402939],\n",
       "       [0.24097556],\n",
       "       [0.24329104],\n",
       "       [0.24553417],\n",
       "       [0.24400492],\n",
       "       [0.25004655],\n",
       "       [0.24715891],\n",
       "       [0.24821438],\n",
       "       [0.25191495],\n",
       "       [0.25588312],\n",
       "       [0.25535642],\n",
       "       [0.25933027],\n",
       "       [0.26273236],\n",
       "       [0.2666943 ],\n",
       "       [0.2656474 ],\n",
       "       [0.26553092],\n",
       "       [0.26598212],\n",
       "       [0.2639791 ],\n",
       "       [0.25922795],\n",
       "       [0.25442737],\n",
       "       [0.25119162],\n",
       "       [0.25104504],\n",
       "       [0.2521064 ],\n",
       "       [0.24976781],\n",
       "       [0.251115  ],\n",
       "       [0.25387967],\n",
       "       [0.25244703],\n",
       "       [0.24860246],\n",
       "       [0.25421253],\n",
       "       [0.24755792],\n",
       "       [0.25421308],\n",
       "       [0.26263231],\n",
       "       [0.27745641],\n",
       "       [0.29444483],\n",
       "       [0.32447995],\n",
       "       [0.345848  ],\n",
       "       [0.36037326],\n",
       "       [0.36783332],\n",
       "       [0.37344444],\n",
       "       [0.37477964],\n",
       "       [0.37468156],\n",
       "       [0.37293986],\n",
       "       [0.37471306],\n",
       "       [0.3759316 ],\n",
       "       [0.37851205],\n",
       "       [0.38689246],\n",
       "       [0.38950748],\n",
       "       [0.38447339],\n",
       "       [0.38440063],\n",
       "       [0.37721867],\n",
       "       [0.36709572],\n",
       "       [0.3677577 ],\n",
       "       [0.37335385],\n",
       "       [0.37579832],\n",
       "       [0.3807152 ],\n",
       "       [0.38387128],\n",
       "       [0.38541432],\n",
       "       [0.38015079],\n",
       "       [0.38359794],\n",
       "       [0.37893417],\n",
       "       [0.38806243],\n",
       "       [0.39043983],\n",
       "       [0.39336021],\n",
       "       [0.39162867],\n",
       "       [0.3964081 ],\n",
       "       [0.39818655],\n",
       "       [0.39991151],\n",
       "       [0.39798613],\n",
       "       [0.38951715],\n",
       "       [0.40046415],\n",
       "       [0.45648353],\n",
       "       [0.56012954],\n",
       "       [0.66671378],\n",
       "       [0.58948573],\n",
       "       [0.75968974],\n",
       "       [0.89128848],\n",
       "       [0.92000408],\n",
       "       [0.94621018],\n",
       "       [0.95165507],\n",
       "       [0.97037815],\n",
       "       [0.98355786],\n",
       "       [0.95341745],\n",
       "       [0.94104844],\n",
       "       [0.95077043],\n",
       "       [0.93037272],\n",
       "       [0.95242303],\n",
       "       [0.9396683 ],\n",
       "       [0.92382958],\n",
       "       [0.93231759],\n",
       "       [0.90585679],\n",
       "       [0.90371477],\n",
       "       [0.88164447],\n",
       "       [0.87088916],\n",
       "       [0.87464293],\n",
       "       [0.86529818],\n",
       "       [0.87578625],\n",
       "       [0.87752292],\n",
       "       [0.86755509],\n",
       "       [0.87802244],\n",
       "       [0.89707968],\n",
       "       [0.92637404],\n",
       "       [0.93392308],\n",
       "       [0.97774696],\n",
       "       [1.0076382 ],\n",
       "       [1.08307236],\n",
       "       [1.14095578],\n",
       "       [1.18032777],\n",
       "       [1.21408636],\n",
       "       [1.26567204],\n",
       "       [1.33893688],\n",
       "       [1.36766982],\n",
       "       [1.40505291],\n",
       "       [1.43561904],\n",
       "       [1.46002932],\n",
       "       [1.45114632],\n",
       "       [1.45867351],\n",
       "       [1.46990248],\n",
       "       [1.45804011],\n",
       "       [1.44147357],\n",
       "       [1.40215773],\n",
       "       [1.37327227],\n",
       "       [1.32199914],\n",
       "       [1.30723698],\n",
       "       [1.30337961],\n",
       "       [1.29879639],\n",
       "       [1.30039704],\n",
       "       [1.30159737],\n",
       "       [1.26988555],\n",
       "       [1.20783112],\n",
       "       [1.1572139 ],\n",
       "       [1.08248968],\n",
       "       [1.0086431 ],\n",
       "       [0.96716875],\n",
       "       [0.99715148],\n",
       "       [0.98315228],\n",
       "       [1.0121903 ],\n",
       "       [1.04385694],\n",
       "       [1.05240571],\n",
       "       [1.06785789],\n",
       "       [1.08231687],\n",
       "       [1.09830324],\n",
       "       [1.10606951],\n",
       "       [1.09539657],\n",
       "       [1.09085636],\n",
       "       [1.10188782],\n",
       "       [1.12491845],\n",
       "       [1.13147498],\n",
       "       [1.09972882],\n",
       "       [1.09697415],\n",
       "       [1.07462082],\n",
       "       [1.07260897],\n",
       "       [1.04607196],\n",
       "       [1.02363823],\n",
       "       [0.972321  ],\n",
       "       [0.94398086],\n",
       "       [0.96124941],\n",
       "       [0.94775467],\n",
       "       [0.91642564],\n",
       "       [0.86724345],\n",
       "       [0.83697626],\n",
       "       [0.80645942],\n",
       "       [0.8249708 ],\n",
       "       [0.82619786],\n",
       "       [0.84172426],\n",
       "       [0.8411175 ],\n",
       "       [0.85959974],\n",
       "       [0.86903684],\n",
       "       [0.84944353],\n",
       "       [0.83880347],\n",
       "       [0.81713   ],\n",
       "       [0.81866207],\n",
       "       [0.83097251],\n",
       "       [0.84080502],\n",
       "       [0.85176472],\n",
       "       [0.84745403],\n",
       "       [0.84089617],\n",
       "       [0.84027176],\n",
       "       [0.84527772],\n",
       "       [0.85527256],\n",
       "       [0.86881528],\n",
       "       [0.87599107],\n",
       "       [0.8742754 ],\n",
       "       [0.8701125 ],\n",
       "       [0.87529293],\n",
       "       [0.87245816],\n",
       "       [0.86875601],\n",
       "       [0.86954599],\n",
       "       [0.86703786],\n",
       "       [0.86013281],\n",
       "       [0.85254281],\n",
       "       [0.85099562],\n",
       "       [0.84663221],\n",
       "       [0.83996744],\n",
       "       [0.84066047],\n",
       "       [0.84101757],\n",
       "       [0.84086159],\n",
       "       [0.84066576],\n",
       "       [0.83973529],\n",
       "       [0.83245005],\n",
       "       [0.82577757],\n",
       "       [0.82262617],\n",
       "       [0.81526706],\n",
       "       [0.80838017],\n",
       "       [0.79466193],\n",
       "       [0.78285191],\n",
       "       [0.77739481],\n",
       "       [0.76608826],\n",
       "       [0.7497018 ],\n",
       "       [0.7326444 ],\n",
       "       [0.72524334],\n",
       "       [0.7194476 ],\n",
       "       [0.72022199],\n",
       "       [0.71410368],\n",
       "       [0.71636894],\n",
       "       [0.70580421],\n",
       "       [0.70749805],\n",
       "       [0.7073922 ],\n",
       "       [0.70586058],\n",
       "       [0.7039369 ],\n",
       "       [0.70430717],\n",
       "       [0.71075762],\n",
       "       [0.71610288],\n",
       "       [0.71525131],\n",
       "       [0.72437831],\n",
       "       [0.72640416],\n",
       "       [0.73539993],\n",
       "       [0.73454486],\n",
       "       [0.73486225],\n",
       "       [0.73176781],\n",
       "       [0.72273692],\n",
       "       [0.72576473],\n",
       "       [0.71608259],\n",
       "       [0.71782926],\n",
       "       [0.71066595],\n",
       "       [0.6985817 ],\n",
       "       [0.69092564],\n",
       "       [0.67400475],\n",
       "       [0.66314722],\n",
       "       [0.65064578],\n",
       "       [0.63854481],\n",
       "       [0.62434141],\n",
       "       [0.60881203],\n",
       "       [0.59201698],\n",
       "       [0.58219336],\n",
       "       [0.56669754],\n",
       "       [0.54732764],\n",
       "       [0.53396125],\n",
       "       [0.51488862],\n",
       "       [0.49335998],\n",
       "       [0.47678678],\n",
       "       [0.4586368 ],\n",
       "       [0.44851234],\n",
       "       [0.4326586 ],\n",
       "       [0.41986119],\n",
       "       [0.40739336],\n",
       "       [0.39656656],\n",
       "       [0.38627579],\n",
       "       [0.37467242],\n",
       "       [0.35905705],\n",
       "       [0.34436512],\n",
       "       [0.32641741],\n",
       "       [0.31431139],\n",
       "       [0.30552543],\n",
       "       [0.29392586],\n",
       "       [0.28886459],\n",
       "       [0.28723869],\n",
       "       [0.28287393],\n",
       "       [0.2782768 ],\n",
       "       [0.27804969],\n",
       "       [0.28112855],\n",
       "       [0.28214367],\n",
       "       [0.29874264],\n",
       "       [0.30945367],\n",
       "       [0.31971218],\n",
       "       [0.32849472],\n",
       "       [0.34239076],\n",
       "       [0.35362343],\n",
       "       [0.35736049],\n",
       "       [0.3590597 ],\n",
       "       [0.3611719 ],\n",
       "       [0.3472975 ],\n",
       "       [0.30154522],\n",
       "       [0.29747905],\n",
       "       [0.29559631],\n",
       "       [0.29254782],\n",
       "       [0.28671516],\n",
       "       [0.28037911],\n",
       "       [0.27699204],\n",
       "       [0.27615872],\n",
       "       [0.27747526],\n",
       "       [0.27364989],\n",
       "       [0.25975383],\n",
       "       [0.24529342],\n",
       "       [0.24272082],\n",
       "       [0.23991705],\n",
       "       [0.25208213],\n",
       "       [0.25697501],\n",
       "       [0.2564226 ],\n",
       "       [0.25181915],\n",
       "       [0.24051035],\n",
       "       [0.23735709],\n",
       "       [0.22537026],\n",
       "       [0.22420348],\n",
       "       [0.22593844],\n",
       "       [0.22685198],\n",
       "       [0.22530143],\n",
       "       [0.21925737],\n",
       "       [0.2185362 ],\n",
       "       [0.21874722],\n",
       "       [0.22141366],\n",
       "       [0.22402928],\n",
       "       [0.22870922],\n",
       "       [0.22763622],\n",
       "       [0.2273367 ],\n",
       "       [0.22658559],\n",
       "       [0.23277801],\n",
       "       [0.23482995],\n",
       "       [0.23700558],\n",
       "       [0.23966972],\n",
       "       [0.24111363],\n",
       "       [0.24227063],\n",
       "       [0.24588842],\n",
       "       [0.24773842],\n",
       "       [0.25030314],\n",
       "       [0.24812901],\n",
       "       [0.24398008],\n",
       "       [0.24106628],\n",
       "       [0.23450829],\n",
       "       [0.23044367],\n",
       "       [0.22671611],\n",
       "       [0.22135242],\n",
       "       [0.22043265],\n",
       "       [0.21713258],\n",
       "       [0.21569138],\n",
       "       [0.21676137],\n",
       "       [0.21092267],\n",
       "       [0.20947708],\n",
       "       [0.20721755],\n",
       "       [0.2047169 ],\n",
       "       [0.20379393],\n",
       "       [0.2056508 ],\n",
       "       [0.20646475],\n",
       "       [0.20679043],\n",
       "       [0.20837628],\n",
       "       [0.21427837],\n",
       "       [0.21541473],\n",
       "       [0.21777708],\n",
       "       [0.21749955],\n",
       "       [0.21436541],\n",
       "       [0.21479388],\n",
       "       [0.21593916],\n",
       "       [0.2154254 ],\n",
       "       [0.22364873],\n",
       "       [0.22385388],\n",
       "       [0.22258347],\n",
       "       [0.22095006],\n",
       "       [0.21788266],\n",
       "       [0.21906499],\n",
       "       [0.21888807],\n",
       "       [0.21979637],\n",
       "       [0.21924736],\n",
       "       [0.21882565],\n",
       "       [0.22124783],\n",
       "       [0.21904947],\n",
       "       [0.21924127],\n",
       "       [0.21655286],\n",
       "       [0.2085902 ],\n",
       "       [0.20665151],\n",
       "       [0.20006429],\n",
       "       [0.19603848],\n",
       "       [0.1949823 ],\n",
       "       [0.19283514],\n",
       "       [0.19156493],\n",
       "       [0.19091003],\n",
       "       [0.19283105],\n",
       "       [0.19214631],\n",
       "       [0.19299392],\n",
       "       [0.19228274],\n",
       "       [0.19500543],\n",
       "       [0.19132383],\n",
       "       [0.19134045],\n",
       "       [0.18709973],\n",
       "       [0.18930436],\n",
       "       [0.18972937],\n",
       "       [0.18927546],\n",
       "       [0.18950188],\n",
       "       [0.18584165],\n",
       "       [0.18736213],\n",
       "       [0.18751815],\n",
       "       [0.19401245],\n",
       "       [0.20031926],\n",
       "       [0.2058537 ],\n",
       "       [0.20587324],\n",
       "       [0.20913645],\n",
       "       [0.21377314],\n",
       "       [0.20790652],\n",
       "       [0.20616632],\n",
       "       [0.20654645],\n",
       "       [0.20280995],\n",
       "       [0.20235145],\n",
       "       [0.19782894],\n",
       "       [0.19705767],\n",
       "       [0.19941204],\n",
       "       [0.19593956],\n",
       "       [0.19520443],\n",
       "       [0.19332002],\n",
       "       [0.18963316],\n",
       "       [0.18987706],\n",
       "       [0.19001145],\n",
       "       [0.194622  ],\n",
       "       [0.20245163],\n",
       "       [0.20948597],\n",
       "       [0.21280347],\n",
       "       [0.21385271],\n",
       "       [0.21513614],\n",
       "       [0.21298154],\n",
       "       [0.21258307],\n",
       "       [0.21320433],\n",
       "       [0.20609729],\n",
       "       [0.19831433],\n",
       "       [0.19549484],\n",
       "       [0.19818218],\n",
       "       [0.20231991],\n",
       "       [0.20308682],\n",
       "       [0.20602955],\n",
       "       [0.20702654],\n",
       "       [0.20544189],\n",
       "       [0.20684914],\n",
       "       [0.20904278],\n",
       "       [0.21276677],\n",
       "       [0.21954593],\n",
       "       [0.22403561],\n",
       "       [0.22607627],\n",
       "       [0.22307822],\n",
       "       [0.22480475],\n",
       "       [0.2292513 ],\n",
       "       [0.22966948],\n",
       "       [0.22975104],\n",
       "       [0.23338899],\n",
       "       [0.23507261],\n",
       "       [0.23074314],\n",
       "       [0.27158592],\n",
       "       [0.28047112],\n",
       "       [0.24337334],\n",
       "       [0.22969329],\n",
       "       [0.22256249],\n",
       "       [0.20700637],\n",
       "       [0.20332073],\n",
       "       [0.19927941],\n",
       "       [0.20184159],\n",
       "       [0.20531367],\n",
       "       [0.20617244],\n",
       "       [0.20709149],\n",
       "       [0.20817923],\n",
       "       [0.20865274],\n",
       "       [0.21246472],\n",
       "       [0.21956641],\n",
       "       [0.2218752 ],\n",
       "       [0.22543361],\n",
       "       [0.22410746],\n",
       "       [0.22986346],\n",
       "       [0.22969085],\n",
       "       [0.23244189],\n",
       "       [0.23012223],\n",
       "       [0.22968544],\n",
       "       [0.22411553],\n",
       "       [0.2220908 ],\n",
       "       [0.23193221],\n",
       "       [0.2507777 ],\n",
       "       [0.25676221],\n",
       "       [0.25825383],\n",
       "       [0.25254183],\n",
       "       [0.25801996],\n",
       "       [0.25221884],\n",
       "       [0.23987441],\n",
       "       [0.23251102],\n",
       "       [0.22291586],\n",
       "       [0.22348657],\n",
       "       [0.21200241],\n",
       "       [0.20626096],\n",
       "       [0.20559263],\n",
       "       [0.19848979],\n",
       "       [0.19839355],\n",
       "       [0.19625186],\n",
       "       [0.194401  ],\n",
       "       [0.19399491],\n",
       "       [0.19312524],\n",
       "       [0.19525945],\n",
       "       [0.19622021],\n",
       "       [0.19716272],\n",
       "       [0.19669085],\n",
       "       [0.19664063],\n",
       "       [0.19598171],\n",
       "       [0.19560644],\n",
       "       [0.19761932],\n",
       "       [0.19506267],\n",
       "       [0.19548826],\n",
       "       [0.19489356],\n",
       "       [0.19830418],\n",
       "       [0.20591984],\n",
       "       [0.20843563],\n",
       "       [0.21398917],\n",
       "       [0.21897679],\n",
       "       [0.22430827],\n",
       "       [0.22993812],\n",
       "       [0.22959835],\n",
       "       [0.22962066],\n",
       "       [0.23242437],\n",
       "       [0.23482294],\n",
       "       [0.23544704],\n",
       "       [0.23469066],\n",
       "       [0.23618093],\n",
       "       [0.23445596],\n",
       "       [0.2332325 ],\n",
       "       [0.23343119],\n",
       "       [0.23082257],\n",
       "       [0.23194835],\n",
       "       [0.23262014],\n",
       "       [0.23430934],\n",
       "       [0.23374424],\n",
       "       [0.22943334],\n",
       "       [0.22888239],\n",
       "       [0.23143346],\n",
       "       [0.2314    ],\n",
       "       [0.23380009],\n",
       "       [0.23282081],\n",
       "       [0.23280507],\n",
       "       [0.23431619],\n",
       "       [0.23427501],\n",
       "       [0.23626209],\n",
       "       [0.2397049 ],\n",
       "       [0.24353976],\n",
       "       [0.24298245],\n",
       "       [0.24322161],\n",
       "       [0.24605518],\n",
       "       [0.24902105],\n",
       "       [0.24408473],\n",
       "       [0.23869915],\n",
       "       [0.23441109],\n",
       "       [0.22806279],\n",
       "       [0.22545143],\n",
       "       [0.22080483],\n",
       "       [0.21954186],\n",
       "       [0.21321801],\n",
       "       [0.21094157]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mae_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62336c29-eb53-48bd-81e2-6d225604f54b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.24583492, 0.24530709, 0.23988283, 0.23530061, 0.23466073,\n",
       "       0.23211747, 0.22691829, 0.22562859, 0.2221147 , 0.22206621,\n",
       "       0.21734948, 0.21369621, 0.21219807, 0.20530635, 0.20925745,\n",
       "       0.20922043, 0.21850484, 0.22283909, 0.22506483, 0.2240477 ,\n",
       "       0.21628623, 0.21456709, 0.21228801, 0.21469267, 0.21637185,\n",
       "       0.2160269 , 0.21514701, 0.21252809, 0.21157097, 0.20998351,\n",
       "       0.20972647, 0.21158937, 0.21450477, 0.21988985, 0.21789274,\n",
       "       0.21321649, 0.21044902, 0.21064248, 0.21048559, 0.20325651,\n",
       "       0.19998314, 0.20609971, 0.20299504, 0.19953579, 0.20270707,\n",
       "       0.21126766, 0.21492309, 0.21378971, 0.2146587 , 0.21433214,\n",
       "       0.21472407, 0.21683909, 0.22236155, 0.22167039, 0.22548078,\n",
       "       0.23255708, 0.23167951, 0.22859828, 0.23185366, 0.2356097 ,\n",
       "       0.23472874, 0.23392556, 0.23883786, 0.23868654, 0.23915626,\n",
       "       0.24214324, 0.24725294, 0.25227506, 0.26161532, 0.26614272,\n",
       "       0.27206954, 0.2761991 , 0.27556014, 0.27606891, 0.27951069,\n",
       "       0.28270209, 0.27934656, 0.27622293, 0.26516228, 0.2884754 ,\n",
       "       0.29263332, 0.28255872, 0.26825445, 0.25576166, 0.2431399 ,\n",
       "       0.23584568, 0.23243665, 0.22819484, 0.2260979 , 0.22332212,\n",
       "       0.2206949 , 0.21980879, 0.22015464, 0.21741501, 0.22251861,\n",
       "       0.23209467, 0.23299322, 0.2420725 , 0.24635127, 0.24331325,\n",
       "       0.24367581, 0.24515444, 0.24807041, 0.25170487, 0.25144939,\n",
       "       0.24849369, 0.24685372, 0.24621791, 0.24916162, 0.25124306,\n",
       "       0.24729674, 0.24821025, 0.24813955, 0.24350111, 0.24218581,\n",
       "       0.23930499, 0.23812955, 0.23048722, 0.22639548, 0.2232339 ,\n",
       "       0.21866891, 0.21904719, 0.21693146, 0.22187905, 0.22215839,\n",
       "       0.22164213, 0.22101005, 0.21917374, 0.22054873, 0.22262033,\n",
       "       0.21890063, 0.21896111, 0.21702823, 0.21639125, 0.22093521,\n",
       "       0.22286772, 0.22725453, 0.23148774, 0.23482512, 0.23714346,\n",
       "       0.23997237, 0.24533949, 0.25130187, 0.25251844, 0.24986269,\n",
       "       0.24951943, 0.24930309, 0.25046706, 0.2474835 , 0.24661469,\n",
       "       0.24822193, 0.24541725, 0.24565194, 0.24440719, 0.24082239,\n",
       "       0.24050933, 0.24015024, 0.2371234 , 0.23663923, 0.23766316,\n",
       "       0.23846875, 0.23818705, 0.24422367, 0.24695889, 0.2489544 ,\n",
       "       0.25042682, 0.24977987, 0.24800463, 0.24701591, 0.2459488 ,\n",
       "       0.24505243, 0.24572723, 0.2438051 , 0.24516964, 0.24560184,\n",
       "       0.24497079, 0.24767623, 0.24627876, 0.24587139, 0.24404297,\n",
       "       0.23106815, 0.22393966, 0.21671518, 0.22028999, 0.22491005,\n",
       "       0.23296133, 0.2344555 , 0.23235987, 0.22946367, 0.22935227,\n",
       "       0.22415006, 0.22130156, 0.22274906, 0.22558099, 0.23012984,\n",
       "       0.2335522 , 0.23595448, 0.23508883, 0.23627968, 0.23661275,\n",
       "       0.24206982, 0.23615388, 0.23479308, 0.22914669, 0.2255309 ,\n",
       "       0.22399697, 0.22077889, 0.2180129 , 0.2177501 , 0.21637722,\n",
       "       0.21805972, 0.21774036, 0.21870359, 0.21684859, 0.21690089,\n",
       "       0.21797954, 0.222215  , 0.22633535, 0.22986393, 0.23049056,\n",
       "       0.23407483, 0.23820138, 0.24248909, 0.24518668, 0.24871482,\n",
       "       0.25305854, 0.25337559, 0.25195261, 0.2498157 , 0.24672973,\n",
       "       0.23816626, 0.23362109, 0.22678821, 0.22086246, 0.21904081,\n",
       "       0.2196389 , 0.2229463 , 0.22434159, 0.22793454, 0.23565654,\n",
       "       0.24092513, 0.24402681, 0.24258761, 0.23963409, 0.24000005,\n",
       "       0.23653359, 0.237586  , 0.24197012, 0.24727083, 0.25911312,\n",
       "       0.2671595 , 0.27137573, 0.27240104, 0.27470817, 0.27803865,\n",
       "       0.24425241, 0.2316367 , 0.23742143, 0.23627464, 0.23427205,\n",
       "       0.23474633, 0.23555794, 0.23364316, 0.24291839, 0.24750041,\n",
       "       0.24596011, 0.24276616, 0.23985765, 0.24249205, 0.26381679,\n",
       "       0.26744155, 0.26766654, 0.26356075, 0.25751391, 0.24993937,\n",
       "       0.24875363, 0.24760077, 0.24209812, 0.23691033, 0.23442248,\n",
       "       0.23210404, 0.23841769, 0.24663052, 0.24647485, 0.24546984,\n",
       "       0.24541421, 0.24425014, 0.24510568, 0.2461861 , 0.24813819,\n",
       "       0.24937961, 0.24877499, 0.24994065, 0.24627551, 0.24141346,\n",
       "       0.241536  , 0.24519804, 0.24957053, 0.25560468, 0.25244667,\n",
       "       0.25689308, 0.25572962, 0.25770097, 0.26493004, 0.25663622,\n",
       "       0.26001047, 0.25134639, 0.24210942, 0.24402939, 0.24097556,\n",
       "       0.24329104, 0.24553417, 0.24400492, 0.25004655, 0.24715891,\n",
       "       0.24821438, 0.25191495, 0.25588312, 0.25535642, 0.25933027,\n",
       "       0.26273236, 0.2666943 , 0.2656474 , 0.26553092, 0.26598212,\n",
       "       0.2639791 , 0.25922795, 0.25442737, 0.25119162, 0.25104504,\n",
       "       0.2521064 , 0.24976781, 0.251115  , 0.25387967, 0.25244703,\n",
       "       0.24860246, 0.25421253, 0.24755792, 0.25421308, 0.26263231,\n",
       "       0.27745641, 0.29444483, 0.32447995, 0.345848  , 0.36037326,\n",
       "       0.36783332, 0.37344444, 0.37477964, 0.37468156, 0.37293986,\n",
       "       0.37471306, 0.3759316 , 0.37851205, 0.38689246, 0.38950748,\n",
       "       0.38447339, 0.38440063, 0.37721867, 0.36709572, 0.3677577 ,\n",
       "       0.37335385, 0.37579832, 0.3807152 , 0.38387128, 0.38541432,\n",
       "       0.38015079, 0.38359794, 0.37893417, 0.38806243, 0.39043983,\n",
       "       0.39336021, 0.39162867, 0.3964081 , 0.39818655, 0.39991151,\n",
       "       0.39798613, 0.38951715, 0.40046415, 0.45648353, 0.56012954,\n",
       "       0.66671378, 0.58948573, 0.75968974, 0.89128848, 0.92000408,\n",
       "       0.94621018, 0.95165507, 0.97037815, 0.98355786, 0.95341745,\n",
       "       0.94104844, 0.95077043, 0.93037272, 0.95242303, 0.9396683 ,\n",
       "       0.92382958, 0.93231759, 0.90585679, 0.90371477, 0.88164447,\n",
       "       0.87088916, 0.87464293, 0.86529818, 0.87578625, 0.87752292,\n",
       "       0.86755509, 0.87802244, 0.89707968, 0.92637404, 0.93392308,\n",
       "       0.97774696, 1.0076382 , 1.08307236, 1.14095578, 1.18032777,\n",
       "       1.21408636, 1.26567204, 1.33893688, 1.36766982, 1.40505291,\n",
       "       1.43561904, 1.46002932, 1.45114632, 1.45867351, 1.46990248,\n",
       "       1.45804011, 1.44147357, 1.40215773, 1.37327227, 1.32199914,\n",
       "       1.30723698, 1.30337961, 1.29879639, 1.30039704, 1.30159737,\n",
       "       1.26988555, 1.20783112, 1.1572139 , 1.08248968, 1.0086431 ,\n",
       "       0.96716875, 0.99715148, 0.98315228, 1.0121903 , 1.04385694,\n",
       "       1.05240571, 1.06785789, 1.08231687, 1.09830324, 1.10606951,\n",
       "       1.09539657, 1.09085636, 1.10188782, 1.12491845, 1.13147498,\n",
       "       1.09972882, 1.09697415, 1.07462082, 1.07260897, 1.04607196,\n",
       "       1.02363823, 0.972321  , 0.94398086, 0.96124941, 0.94775467,\n",
       "       0.91642564, 0.86724345, 0.83697626, 0.80645942, 0.8249708 ,\n",
       "       0.82619786, 0.84172426, 0.8411175 , 0.85959974, 0.86903684,\n",
       "       0.84944353, 0.83880347, 0.81713   , 0.81866207, 0.83097251,\n",
       "       0.84080502, 0.85176472, 0.84745403, 0.84089617, 0.84027176,\n",
       "       0.84527772, 0.85527256, 0.86881528, 0.87599107, 0.8742754 ,\n",
       "       0.8701125 , 0.87529293, 0.87245816, 0.86875601, 0.86954599,\n",
       "       0.86703786, 0.86013281, 0.85254281, 0.85099562, 0.84663221,\n",
       "       0.83996744, 0.84066047, 0.84101757, 0.84086159, 0.84066576,\n",
       "       0.83973529, 0.83245005, 0.82577757, 0.82262617, 0.81526706,\n",
       "       0.80838017, 0.79466193, 0.78285191, 0.77739481, 0.76608826,\n",
       "       0.7497018 , 0.7326444 , 0.72524334, 0.7194476 , 0.72022199,\n",
       "       0.71410368, 0.71636894, 0.70580421, 0.70749805, 0.7073922 ,\n",
       "       0.70586058, 0.7039369 , 0.70430717, 0.71075762, 0.71610288,\n",
       "       0.71525131, 0.72437831, 0.72640416, 0.73539993, 0.73454486,\n",
       "       0.73486225, 0.73176781, 0.72273692, 0.72576473, 0.71608259,\n",
       "       0.71782926, 0.71066595, 0.6985817 , 0.69092564, 0.67400475,\n",
       "       0.66314722, 0.65064578, 0.63854481, 0.62434141, 0.60881203,\n",
       "       0.59201698, 0.58219336, 0.56669754, 0.54732764, 0.53396125,\n",
       "       0.51488862, 0.49335998, 0.47678678, 0.4586368 , 0.44851234,\n",
       "       0.4326586 , 0.41986119, 0.40739336, 0.39656656, 0.38627579,\n",
       "       0.37467242, 0.35905705, 0.34436512, 0.32641741, 0.31431139,\n",
       "       0.30552543, 0.29392586, 0.28886459, 0.28723869, 0.28287393,\n",
       "       0.2782768 , 0.27804969, 0.28112855, 0.28214367, 0.29874264,\n",
       "       0.30945367, 0.31971218, 0.32849472, 0.34239076, 0.35362343,\n",
       "       0.35736049, 0.3590597 , 0.3611719 , 0.3472975 , 0.30154522,\n",
       "       0.29747905, 0.29559631, 0.29254782, 0.28671516, 0.28037911,\n",
       "       0.27699204, 0.27615872, 0.27747526, 0.27364989, 0.25975383,\n",
       "       0.24529342, 0.24272082, 0.23991705, 0.25208213, 0.25697501,\n",
       "       0.2564226 , 0.25181915, 0.24051035, 0.23735709, 0.22537026,\n",
       "       0.22420348, 0.22593844, 0.22685198, 0.22530143, 0.21925737,\n",
       "       0.2185362 , 0.21874722, 0.22141366, 0.22402928, 0.22870922,\n",
       "       0.22763622, 0.2273367 , 0.22658559, 0.23277801, 0.23482995,\n",
       "       0.23700558, 0.23966972, 0.24111363, 0.24227063, 0.24588842,\n",
       "       0.24773842, 0.25030314, 0.24812901, 0.24398008, 0.24106628,\n",
       "       0.23450829, 0.23044367, 0.22671611, 0.22135242, 0.22043265,\n",
       "       0.21713258, 0.21569138, 0.21676137, 0.21092267, 0.20947708,\n",
       "       0.20721755, 0.2047169 , 0.20379393, 0.2056508 , 0.20646475,\n",
       "       0.20679043, 0.20837628, 0.21427837, 0.21541473, 0.21777708,\n",
       "       0.21749955, 0.21436541, 0.21479388, 0.21593916, 0.2154254 ,\n",
       "       0.22364873, 0.22385388, 0.22258347, 0.22095006, 0.21788266,\n",
       "       0.21906499, 0.21888807, 0.21979637, 0.21924736, 0.21882565,\n",
       "       0.22124783, 0.21904947, 0.21924127, 0.21655286, 0.2085902 ,\n",
       "       0.20665151, 0.20006429, 0.19603848, 0.1949823 , 0.19283514,\n",
       "       0.19156493, 0.19091003, 0.19283105, 0.19214631, 0.19299392,\n",
       "       0.19228274, 0.19500543, 0.19132383, 0.19134045, 0.18709973,\n",
       "       0.18930436, 0.18972937, 0.18927546, 0.18950188, 0.18584165,\n",
       "       0.18736213, 0.18751815, 0.19401245, 0.20031926, 0.2058537 ,\n",
       "       0.20587324, 0.20913645, 0.21377314, 0.20790652, 0.20616632,\n",
       "       0.20654645, 0.20280995, 0.20235145, 0.19782894, 0.19705767,\n",
       "       0.19941204, 0.19593956, 0.19520443, 0.19332002, 0.18963316,\n",
       "       0.18987706, 0.19001145, 0.194622  , 0.20245163, 0.20948597,\n",
       "       0.21280347, 0.21385271, 0.21513614, 0.21298154, 0.21258307,\n",
       "       0.21320433, 0.20609729, 0.19831433, 0.19549484, 0.19818218,\n",
       "       0.20231991, 0.20308682, 0.20602955, 0.20702654, 0.20544189,\n",
       "       0.20684914, 0.20904278, 0.21276677, 0.21954593, 0.22403561,\n",
       "       0.22607627, 0.22307822, 0.22480475, 0.2292513 , 0.22966948,\n",
       "       0.22975104, 0.23338899, 0.23507261, 0.23074314, 0.27158592,\n",
       "       0.28047112, 0.24337334, 0.22969329, 0.22256249, 0.20700637,\n",
       "       0.20332073, 0.19927941, 0.20184159, 0.20531367, 0.20617244,\n",
       "       0.20709149, 0.20817923, 0.20865274, 0.21246472, 0.21956641,\n",
       "       0.2218752 , 0.22543361, 0.22410746, 0.22986346, 0.22969085,\n",
       "       0.23244189, 0.23012223, 0.22968544, 0.22411553, 0.2220908 ,\n",
       "       0.23193221, 0.2507777 , 0.25676221, 0.25825383, 0.25254183,\n",
       "       0.25801996, 0.25221884, 0.23987441, 0.23251102, 0.22291586,\n",
       "       0.22348657, 0.21200241, 0.20626096, 0.20559263, 0.19848979,\n",
       "       0.19839355, 0.19625186, 0.194401  , 0.19399491, 0.19312524,\n",
       "       0.19525945, 0.19622021, 0.19716272, 0.19669085, 0.19664063,\n",
       "       0.19598171, 0.19560644, 0.19761932, 0.19506267, 0.19548826,\n",
       "       0.19489356, 0.19830418, 0.20591984, 0.20843563, 0.21398917,\n",
       "       0.21897679, 0.22430827, 0.22993812, 0.22959835, 0.22962066,\n",
       "       0.23242437, 0.23482294, 0.23544704, 0.23469066, 0.23618093,\n",
       "       0.23445596, 0.2332325 , 0.23343119, 0.23082257, 0.23194835,\n",
       "       0.23262014, 0.23430934, 0.23374424, 0.22943334, 0.22888239,\n",
       "       0.23143346, 0.2314    , 0.23380009, 0.23282081, 0.23280507,\n",
       "       0.23431619, 0.23427501, 0.23626209, 0.2397049 , 0.24353976,\n",
       "       0.24298245, 0.24322161, 0.24605518, 0.24902105, 0.24408473,\n",
       "       0.23869915, 0.23441109, 0.22806279, 0.22545143, 0.22080483,\n",
       "       0.21954186, 0.21321801, 0.21094157])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mae_loss = np.mean(test_mae_loss, axis=1)\n",
    "test_mae_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e8f6549-c76a-4626-ad20-e955749dd764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_mae_loss = test_mae_loss.reshape((-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7415ae05-bf45-4247-a8ac-12984f50f206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False,  True,  True,  True,  True, False, False, False,\n",
       "       False, False,  True,  True,  True,  True,  True, False,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anomalies = test_mae_loss > training_var.threshold\n",
    "anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d26b078-2679-469e-870e-d07830401d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw anomaly samples:  [353 354 355 356 362 363 364 365 366 368 369 370 371 372 373 374 375 376\n",
      " 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394\n",
      " 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412\n",
      " 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430\n",
      " 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448\n",
      " 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466\n",
      " 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484\n",
      " 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502\n",
      " 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520\n",
      " 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538\n",
      " 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556\n",
      " 557 558 559 560 561 562 563 564]\n",
      "Number of anomaly samples:  114\n",
      "Indices of anomaly samples:  [494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607]\n"
     ]
    }
   ],
   "source": [
    "anomalous_data_indices = univariate_anomalous_data(anomalies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b12f390-76d9-476e-a3da-97c49226d994",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "48c83d8c-9248-46c1-8e77-a3c0547f6d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test input shape:  (853, 168, 2)\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras import layers\n",
    "from matplotlib import pyplot as plt\n",
    "import training_var\n",
    "import sys\n",
    "def read_dataset(file):\n",
    "    df = pd.read_csv(file, parse_dates=True, index_col=\"timestamp\",header=0)\n",
    "    return df\n",
    "\n",
    "# Generated training sequences for use in the model.\n",
    "def create_sequences(values, time_steps):\n",
    "    output = []\n",
    "    for i in range(len(values) - time_steps + 1):\n",
    "        output.append(values[i : (i + time_steps)])\n",
    "    return np.stack(output)\n",
    "\n",
    "def pre_processing(df,TIME_STEPS):\n",
    "    df = df.dropna()\n",
    "    df_test_value = (df - training_var.training_mean) / training_var.training_std\n",
    "    # Create sequences from test values.\n",
    "    x_test = create_sequences(df_test_value.values,TIME_STEPS)\n",
    "    print(\"Test input shape: \", x_test.shape)\n",
    "    return x_test\n",
    "\n",
    "def univariate_anomalous_data(anomalies):\n",
    "    anomalous_data_indices_raw = np.where(anomalies)[0]\n",
    "    anomalous_data_indices_raw = [x+TIME_STEPS/2 for x in anomalous_data_indices_raw]\n",
    "    anomalous_data_indices = []\n",
    "    for data_idx in anomalous_data_indices_raw:\n",
    "        if set(range(int(data_idx)-int(TIME_STEPS/4),int(data_idx)+int(TIME_STEPS/4))).issubset(set(anomalous_data_indices_raw)):\n",
    "            anomalous_data_indices.append(int(data_idx))\n",
    "    print(\"Number of anomaly samples: \", len(anomalous_data_indices))\n",
    "    print(\"Indices of anomaly samples: \", anomalous_data_indices)\n",
    "    return anomalous_data_indices\n",
    "\n",
    "def generate_res(df,anomalous_data_indices):\n",
    "    df_res = df.reset_index()\n",
    "    df_res['res'] = False\n",
    "    df_res.loc[anomalous_data_indices,'res'] = True\n",
    "    df_res = df_res.filter(['timestamp','res'])\n",
    "    df_res.to_csv(\"../res/res.csv\",index=False)\n",
    "\n",
    "TIME_STEPS = training_var.time_steps\n",
    "df_test = read_dataset('../data/test_multi.csv')\n",
    "# df_test = pd.read_csv('../data/test.csv',parse_dates=True, index_col=\"timestamp\",header=0)\n",
    "x_test = pre_processing(df_test,TIME_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1f2c18e2-a4b4-4741-ac7e-0c02189a2265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 1s 19ms/step\n",
      "Number of anomaly samples:  0\n",
      "Indices of anomaly samples:  []\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model('../res/model.keras')\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# df_test_value.plot(legend=False, ax=ax)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# Get test MAE loss.\n",
    "x_test_pred = model.predict(x_test)\n",
    "test_mae_loss = np.mean(np.abs(x_test_pred - x_test), axis=1)\n",
    "test_mae_loss = np.mean(test_mae_loss, axis=1)\n",
    "# test_mae_loss = test_mae_loss.reshape((-1))\n",
    "\n",
    "# plt.hist(test_mae_loss, bins=50)\n",
    "# plt.xlabel(\"Prediction MAE\")\n",
    "# plt.ylabel(\"No of samples\")\n",
    "# plt.savefig(\"../res/prediction MAE loss.jpg\")\n",
    "\n",
    "# Detect all the samples which are anomalies.\n",
    "anomalies = test_mae_loss > training_var.threshold\n",
    "\n",
    "\n",
    "anomalous_data_indices = univariate_anomalous_data(anomalies)\n",
    "\n",
    "\n",
    "generate_res(df_test,anomalous_data_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c0b367b2-8d1d-4a16-86f8-922967386761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       value1    value2    res\n",
      "timestamp                                     \n",
      "2022-05-31 00:00:00  0.697843  0.697843  False\n",
      "2022-05-31 01:00:00 -2.559581 -2.559581  False\n",
      "2022-05-31 02:00:00  4.155230  4.155230  False\n",
      "2022-05-31 03:00:00  3.568476  3.568476  False\n",
      "2022-05-31 04:00:00  3.213265  3.213265  False\n"
     ]
    }
   ],
   "source": [
    "df_res = df_test\n",
    "df_res['res'] = False\n",
    "df_res.loc[anomalous_data_indices,'res'] = True\n",
    "print(df_res.head())\n",
    "df_res = df_res.filter(['timestamp','res'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "69bfe589-d5e5-488f-8645-bbbfd5d86abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>res</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-05-31 00:00:00</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-31 01:00:00</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-31 02:00:00</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-31 03:00:00</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-31 04:00:00</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-12 07:00:00</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-12 08:00:00</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-12 09:00:00</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-12 10:00:00</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-12 11:00:00</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1020 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       res\n",
       "timestamp                 \n",
       "2022-05-31 00:00:00  False\n",
       "2022-05-31 01:00:00  False\n",
       "2022-05-31 02:00:00  False\n",
       "2022-05-31 03:00:00  False\n",
       "2022-05-31 04:00:00  False\n",
       "...                    ...\n",
       "2022-07-12 07:00:00  False\n",
       "2022-07-12 08:00:00  False\n",
       "2022-07-12 09:00:00  False\n",
       "2022-07-12 10:00:00  False\n",
       "2022-07-12 11:00:00  False\n",
       "\n",
       "[1020 rows x 1 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e048c6-4344-47c5-8d3b-7c63492a207a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
